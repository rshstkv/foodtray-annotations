#!/usr/bin/env tsx
import fs from 'fs'
import path from 'path'
import { createClient } from '@supabase/supabase-js'

function loadEnvFromFile(envPath: string) {
  try {
    if (!fs.existsSync(envPath)) return
    const raw = fs.readFileSync(envPath, 'utf8')
    raw.split(/\r?\n/).forEach(line => {
      const trimmed = line.trim()
      if (!trimmed || trimmed.startsWith('#')) return
      const eqIdx = trimmed.indexOf('=')
      if (eqIdx === -1) return
      const key = trimmed.slice(0, eqIdx).trim()
      let value = trimmed.slice(eqIdx + 1).trim()
      if ((value.startsWith('"') && value.endsWith('"')) || (value.startsWith('\'') && value.endsWith('\''))) {
        value = value.slice(1, -1)
      }
      process.env[key] = value
    })
  } catch (_) {
    // ignore
  }
}

loadEnvFromFile(path.resolve(__dirname, '..', '.env.local'))

const rawUrl = process.env.SUPABASE_URL || process.env.NEXT_PUBLIC_SUPABASE_URL || 'http://127.0.0.1:54321'
const supabaseUrl = rawUrl.replace('localhost', '127.0.0.1').replace(':54323', ':54321')
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY
  || process.env.SUPABASE_ANON_KEY
  || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY
  || ''
if (!supabaseServiceKey) {
  console.warn('SUPABASE_SERVICE_ROLE_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é –∞–Ω–æ–Ω–∏–º–Ω—ã–π –∫–ª—é—á; –ø—Ä–∞–≤–∞ –º–æ–≥—É—Ç –±—ã—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã.')
}
const supabase = createClient(supabaseUrl, supabaseServiceKey)

type State = 'yes' | 'no'

function labelToState(label: string): State | null {
  const t = (label || '').trim().toLowerCase()
  if (t === 'right') return 'yes'
  if (t === 'wrong') return 'no'
  return null
}

function chunkArray<T>(arr: T[], size: number): T[][] {
  const chunks: T[][] = []
  for (let i = 0; i < arr.length; i += size) chunks.push(arr.slice(i, i + size))
  return chunks
}

async function importStates() {
  console.log('üöÄ Starting clarification states import (CSV)...')

  const cliArgPath = process.argv[2]
  if (!cliArgPath) {
    console.error('‚ùå CSV file path is required as first argument')
    return
  }
  const filePath = path.resolve(cliArgPath)
  if (!fs.existsSync(filePath)) {
    console.error('‚ùå File not found:', filePath)
    return
  }

  const raw = fs.readFileSync(filePath, 'utf8')
  const lines = raw.split(/\r?\n/)

  type Row = { clarification_id: string; state: State }
  const rows: Row[] = []
  let skipped = 0

  for (const line of lines) {
    const trimmed = line.trim()
    if (!trimmed) continue
    // –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: –∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, —Ç–∞–±
    const parts = trimmed.split(/[;,\t]/).map(s => s.trim()).filter(Boolean)
    if (parts.length < 2) { skipped++; continue }
    const clarification_id = parts[0]
    const state = labelToState(parts[1])
    if (!clarification_id || !state) { skipped++; continue }
    rows.push({ clarification_id, state })
  }

  // –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç
  const map = new Map<string, Row>()
  for (const r of rows) map.set(r.clarification_id, r)
  const deduped = Array.from(map.values()).map(r => ({
    clarification_id: r.clarification_id,
    state: r.state,
    updated_at: new Date().toISOString()
  }))

  console.log(`üìä Parsed rows: ${rows.length}, unique: ${deduped.length}, skipped: ${skipped}`)

  // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤ clarifications, —á—Ç–æ–±—ã –Ω–µ –Ω–∞—Ä—É—à–∞—Ç—å FK
  const uniqueIds = Array.from(new Set(deduped.map(r => r.clarification_id)))
  const exists = new Set<string>()
  const missing = new Set<string>()
  const chunk = <T,>(arr: T[], size: number): T[][] => {
    const res: T[][] = []
    for (let i = 0; i < arr.length; i += size) res.push(arr.slice(i, i + size))
    return res
  }
  for (const ch of chunk(uniqueIds, 1000)) {
    const { data, error } = await supabase
      .from('clarifications')
      .select('clarification_id')
      .in('clarification_id', ch)
    if (error) {
      console.error('‚ö†Ô∏è Failed to fetch clarifications for existence check:', error)
      // –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è: —Å—á–∏—Ç–∞–µ–º –≤—Å–µ –∫–∞–∫ missing, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –Ω–∞ FK
      ch.forEach(id => missing.add(id))
      continue
    }
    const got = new Set((data || []).map(r => String((r as any).clarification_id)))
    ch.forEach(id => (got.has(id) ? exists.add(id) : missing.add(id)))
  }

  const finalUpserts = deduped.filter(r => exists.has(r.clarification_id))
  const missingIds = Array.from(missing)
  if (missingIds.length) {
    console.warn(`‚ö†Ô∏è ${missingIds.length} clarification_id not found in clarifications; they will be skipped.`)
  }

  let updated = 0
  for (const ch of chunkArray(finalUpserts, 1000)) {
    const { error, data } = await supabase
      .from('clarification_states')
      .upsert(ch, { onConflict: 'clarification_id' })
      .select('clarification_id')
    if (error) {
      console.error('‚ùå Upsert error:', error)
      continue
    }
    updated += data?.length || ch.length
    console.log(`‚úÖ Upserted ${updated}/${finalUpserts.length}`)
  }

  // –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
  const report = {
    parsed: rows.length,
    unique: deduped.length,
    skipped,
    updated,
    missing_count: missingIds.length,
    missing_sample: missingIds.slice(0, 20)
  }
  const reportPath = path.join(__dirname, '..', 'import-states-report.json')
  try {
    fs.writeFileSync(reportPath, JSON.stringify(report, null, 2))
    console.log(`üßæ Report saved: ${reportPath}`)
  } catch (_) {}

  console.log('üéâ States import completed!')
}

importStates().catch(err => {
  console.error('üí• Import failed:', err)
})


