#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ menu_all –∏–∑ —Ñ–∞–π–ª–æ–≤ *_AM.json –Ω–∞–ø—Ä—è–º—É—é –≤ recognitions.menu_all

–£—Å—Ç–æ–π—á–∏–≤ –∫ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º –∑–∞–ø—É—Å–∫–∞–º - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞–∫–∏–µ recognition —É–∂–µ –∏–º–µ—é—Ç menu_all.

Usage:
    python3 scripts/load_menu_all_direct.py \
        --dataset "/Users/romanshestakov/Downloads/RRS_Dataset 2" \
        --env prod
"""

import json
import os
import sys
import argparse
from pathlib import Path
from typing import Set

from supabase import create_client, Client
from dotenv import load_dotenv
from tqdm import tqdm


def setup_supabase(env: str = 'local') -> Client:
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Supabase –∫–ª–∏–µ–Ω—Ç."""
    if env == 'local':
        load_dotenv('.env.local')
    elif env == 'prod':
        load_dotenv('.env.production')
    else:
        raise ValueError(f"Invalid environment: {env}. Use 'local' or 'prod'")
    
    url = os.getenv('SUPABASE_URL') or os.getenv('NEXT_PUBLIC_SUPABASE_URL')
    key = os.getenv('SUPABASE_SERVICE_ROLE_KEY') or os.getenv('SUPABASE_ANON_KEY') or os.getenv('NEXT_PUBLIC_SUPABASE_ANON_KEY')
    
    if not url or not key:
        print(f"‚ùå Error: Supabase credentials not found in .env.{env}")
        sys.exit(1)
    
    print(f"‚úÖ Connected to Supabase ({env}): {url}")
    return create_client(url, key)


def find_export_directory(dataset_dir: Path) -> Path:
    """–ù–∞—Ö–æ–¥–∏—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é export_* –≤–Ω—É—Ç—Ä–∏ dataset_dir."""
    for item in dataset_dir.iterdir():
        if item.is_dir() and item.name.startswith('export_'):
            return item
    raise FileNotFoundError(f"No export_* directory found in {dataset_dir}")


def apply_migration(supabase: Client) -> None:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–∏–≥—Ä–∞—Ü–∏—é –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è menu_all –∫–æ–ª–æ–Ω–∫–∏."""
    print("\nüîß –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∏–≥—Ä–∞—Ü–∏—é...")
    
    migration_sql = """
    ALTER TABLE recognitions ADD COLUMN IF NOT EXISTS menu_all JSONB;
    CREATE INDEX IF NOT EXISTS idx_recognitions_menu_all ON recognitions USING GIN (menu_all);
    """
    
    try:
        # PostgREST API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä—è–º–æ–π SQL, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–≤–µ—Ä–∏–º —á–µ—Ä–µ–∑ SELECT
        result = supabase.table('recognitions').select('menu_all').limit(1).execute()
        print("‚úÖ –ö–æ–ª–æ–Ω–∫–∞ menu_all —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
        print("‚ö†Ô∏è  –ú–∏–≥—Ä–∞—Ü–∏—é –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ Supabase Dashboard")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description='Load menu_all from *_AM.json files to recognitions')
    parser.add_argument('--dataset', required=True, help='Path to RRS_Dataset directory')
    parser.add_argument('--env', choices=['local', 'prod'], default='local', help='Environment (default: local)')
    parser.add_argument('--batch-size', type=int, default=100, help='Batch size for updates (default: 100)')
    args = parser.parse_args()
    
    dataset_dir = Path(args.dataset)
    if not dataset_dir.exists():
        print(f"‚ùå Dataset directory not found: {dataset_dir}")
        sys.exit(1)
    
    # –ù–∞—Ö–æ–¥–∏–º export –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    export_dir = find_export_directory(dataset_dir)
    print(f"üìÅ Found export directory: {export_dir.name}")
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Supabase
    supabase = setup_supabase(args.env)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∏–≥—Ä–∞—Ü–∏—é
    apply_migration(supabase)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º recognitions –∏–∑ –ë–î (–í–°–ï –°–†–ê–ó–£)
    print("\nüì• –ó–∞–≥—Ä—É–∂–∞–µ–º recognitions –∏–∑ –ë–î...")
    all_recognitions = []
    last_id = 0
    batch_size = 1000
    
    while True:
        result = supabase.table('recognitions')\
            .select('recognition_id, menu_all')\
            .gt('recognition_id', last_id)\
            .order('recognition_id')\
            .limit(batch_size)\
            .execute()
        
        if not result.data:
            break
        
        all_recognitions.extend(result.data)
        last_id = result.data[-1]['recognition_id']
        print(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_recognitions)} recognitions...", end='\r')
        
        if len(result.data) < batch_size:
            break
    
    print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_recognitions)} recognitions")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç–µ, —É –∫–æ–≥–æ —É–∂–µ –µ—Å—Ç—å menu_all –∏ —É –∫–æ–≥–æ –Ω–µ—Ç
    with_menu = [r for r in all_recognitions if r.get('menu_all')]
    without_menu = [r for r in all_recognitions if not r.get('menu_all')]
    
    print(f"üìä –° menu_all: {len(with_menu)}, –ë–µ–∑ menu_all: {len(without_menu)}")
    
    if len(without_menu) == 0:
        print("‚úÖ –í—Å–µ recognitions —É–∂–µ –∏–º–µ—é—Ç menu_all!")
        return
    
    # –°–æ–∑–¥–∞–µ–º Set –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    need_update_ids = {r['recognition_id'] for r in without_menu}
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º menu_all –∏–∑ —Ñ–∞–π–ª–æ–≤
    print(f"\nüî® –ó–∞–≥—Ä—É–∂–∞–µ–º menu_all –∏–∑ —Ñ–∞–π–ª–æ–≤ –¥–ª—è {len(need_update_ids)} recognitions...")
    updates = []
    processed = 0
    not_found = 0
    
    for rec_id in tqdm(need_update_ids, desc="Reading files"):
        processed += 1
        
        rec_dir = export_dir / f"recognition_{rec_id}"
        if not rec_dir.exists():
            not_found += 1
            continue
        
        # –ò—â–µ–º —Ñ–∞–π–ª *_AM.json
        am_files = list(rec_dir.glob('*_AM.json'))
        if not am_files:
            not_found += 1
            continue
        
        # –ß–∏—Ç–∞–µ–º menu_all
        try:
            with open(am_files[0], 'r', encoding='utf-8') as f:
                menu_all = json.load(f)
            
            updates.append({
                'recognition_id': rec_id,
                'menu_all': menu_all
            })
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {am_files[0]}: {e}")
            not_found += 1
    
    print(f"\n‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(updates)} updates")
    if not_found > 0:
        print(f"‚ö†Ô∏è  {not_found} recognitions –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è")
    
    if len(updates) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è!")
        return
    
    # Batch update
    print(f"\nüíæ –û–±–Ω–æ–≤–ª—è–µ–º recognitions (batch size: {args.batch_size})...")
    total_updated = 0
    
    for i in tqdm(range(0, len(updates), args.batch_size), desc="Updating"):
        batch = updates[i:i + args.batch_size]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ –æ–¥–Ω–æ–º—É (PostgREST –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç bulk update —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏)
        for update in batch:
            try:
                supabase.table('recognitions')\
                    .update({'menu_all': update['menu_all']})\
                    .eq('recognition_id', update['recognition_id'])\
                    .execute()
                total_updated += 1
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è {update['recognition_id']}: {e}")
    
    print(f"\n{'='*60}")
    print("‚úÖ –ì–û–¢–û–í–û!")
    print(f"{'='*60}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ recognitions: {processed}")
    print(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ: {total_updated}")
    print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ: {not_found}")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()

